{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGZh5NvQJwuZ"
      },
      "source": [
        "### Section 1: Data pipelines\n",
        "\n",
        "Installing required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "OZQirBQ4nGyH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install apache-airflow kubernetes\n",
        "!pip install pyngrok\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting up Airflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "_Vl-URpYJ5jJ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Set up Airflow environment\n",
        "import os\n",
        "os.environ['AIRFLOW_HOME'] = '/content/airflow'\n",
        "\n",
        "# Initialize the airflow database\n",
        "!airflow db init\n",
        "\n",
        "# Create admin user\n",
        "!airflow users create \\\n",
        "    --username junyang \\\n",
        "    --firstname X \\\n",
        "    --lastname Y \\\n",
        "    --role Admin \\\n",
        "    --email admin@example.com \\\n",
        "    --password password\n",
        "\n",
        "# Create DAGs directory\n",
        "!mkdir -p /content/airflow/dags\n",
        "!airflow info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Writing the DAG to the /dags/ folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNW6mK5kKC7z",
        "outputId": "d27f1796-a6d7-4368-9b91-e6aa6e9ccf1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/airflow/dags/section_1_data_pipelines_dag.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/airflow/dags/section_1_data_pipelines_dag.py\n",
        "\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "default_args = {\n",
        "    'start_date': datetime(2024, 1, 22),\n",
        "    'schedule_interval': \"10 1 * * *\",\n",
        "    'retries': 3,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "    'depends_on_past': False,\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'email': 'example@admin.com'\n",
        "}\n",
        "\n",
        "def process_data(input_file_location):\n",
        "  df = pd.read_csv(input_file_location)\n",
        "  #To remove all rows with no name column\n",
        "  df = df.dropna(subset=['name'])\n",
        "  #To split name column up into first_name and last_name columns\n",
        "  df['name'] = df['name'].astype(str)\n",
        "  df[['first_name', 'last_name']] = df['name'].str.split(' ',n=1, expand = True)\n",
        "  #To remove prepended zeros from price column, and convert back to numeric type from string\n",
        "  df['price'] = df['price'].astype(str).str.lstrip('0')\n",
        "  df['price'] = pd.to_numeric(df['price'], errors = 'coerce')\n",
        "  #To create the above_100 column\n",
        "  df['above_100'] = df['price'] > 100\n",
        "  #To drop the original name column\n",
        "  df.drop(columns = ['name'], inplace = True)\n",
        "  original_file_name = os.path.basename(input_file_location)\n",
        "  output_file_name = f'processed_{original_file_name}'\n",
        "  output_path = os.path.join('/content/airflow/dags', output_file_name)\n",
        "  #To output processed CSV files\n",
        "  df.to_csv(output_path, index = False)\n",
        "\n",
        "\n",
        "with DAG('section_1_data_pipelines_dag', default_args = default_args) as dag:\n",
        "\n",
        "  process_dataset1_task = PythonOperator(\n",
        "      task_id = 'process_dataset1_task',\n",
        "      python_callable = process_data,\n",
        "      op_kwargs = {'input_file_location': '/content/airflow/dags/dataset1.csv'}\n",
        "  )\n",
        "\n",
        "  process_dataset2_task = PythonOperator(\n",
        "      task_id = 'process_dataset2_task',\n",
        "      python_callable = process_data,\n",
        "      op_kwargs = {'input_file_location': '/content/airflow/dags/dataset2.csv'}\n",
        "  )\n",
        "\n",
        "process_dataset1_task >> process_dataset2_task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Starting the airflow webserver and scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9a4AKj-UnMl",
        "outputId": "06497918-0145-40c4-f9dc-aac61ac301fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!nohup airflow webserver -p 8888 > webserver.log &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Sm6s5RuZozD",
        "outputId": "bb03af9e-5983-404b-90a6-985220ed9000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ],
      "source": [
        "!nohup airflow scheduler > scheduler.log &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using ngrok to get access the web UI for this instance of Airflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu8mV2QFUsMI",
        "outputId": "4bfba15b-f821-472a-b083-94fd8899b1a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Airflow Web UI is available at: NgrokTunnel: \"https://fd79-34-48-13-194.ngrok-free.app\" -> \"http://localhost:8888\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "#Replace the authtoken with your own authtoken including the pair of single quotes\n",
        "ngrok.set_auth_token('2shIuMDntCePDqOPOBSCV24lQ15_36AfhLcpXwQHffHyyRy8n')\n",
        "\n",
        "public_url = ngrok.connect(8888)\n",
        "print(f\"Airflow Web UI is available at: {public_url}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
